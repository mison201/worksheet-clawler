package main

import (
	"bufio"
	"context"
	"database/sql"
	"encoding/csv"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/temoto/robotstxt"
	_ "modernc.org/sqlite"
)

const (
	baseURL   = "https://www.kiddoworksheets.com"
	listPath  = "/all-downloads/"
	outDir    = "data_kiddo"
	userAgent = "KiddoCrawler-Go/1.0 (+https://example.com/your-contact)"
	rateMs    = 1200 // 1.2s between requests
)

type Meta struct {
	URL     string   `json:"url"`
	Title   string   `json:"title"`
	Subject string   `json:"subject,omitempty"`
	Grades  []string `json:"grades,omitempty"`
	Tags    []string `json:"tags,omitempty"`
	PDFURL  string   `json:"pdf_url,omitempty"`
	PDFPath string   `json:"pdf_path,omitempty"`
}

type Crawler struct {
	client  *http.Client
	robots  *robotstxt.RobotsData
	db      *sql.DB
	csvW    *csv.Writer
	jsonlW  *bufio.Writer
	rate    *time.Ticker
	allowed func(rawURL string) bool
}

func main() {
	if err := run(); err != nil {
		fmt.Fprintf(os.Stderr, "FATAL: %v\n", err)
		os.Exit(1)
	}
}

func run() error {
	if err := os.MkdirAll(outDir, 0o755); err != nil {
		return err
	}

	// Init DB (pure Go SQLite)
	db, err := sql.Open("sqlite", filepath.Join(outDir, "kws.ckpt.sqlite"))
	if err != nil {
		return err
	}
	defer db.Close()

	if err := createSchema(db); err != nil {
		return err
	}

	// CSV / JSONL append mode
	csvFile, err := os.OpenFile(filepath.Join(outDir, "kws_metadata.csv"), os.O_CREATE|os.O_RDWR, 0o644)
	if err != nil {
		return err
	}
	defer csvFile.Close()

	needHeader := isEmpty(csvFile)
	csvW := csv.NewWriter(csvFile)
	if needHeader {
		_ = csvW.Write([]string{"url", "title", "subject", "grades", "tags", "pdf_url", "pdf_path"})
		csvW.Flush()
	}

	jsonlFile, err := os.OpenFile(filepath.Join(outDir, "kws_metadata.jsonl"), os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0o644)
	if err != nil {
		return err
	}
	defer jsonlFile.Close()
	jsonlW := bufio.NewWriter(jsonlFile)

	client := &http.Client{
		Timeout: 30 * time.Second,
		// Follow redirects by default
	}

	robots, allowFn, err := loadRobots(client)
	if err != nil {
		fmt.Fprintf(os.Stderr, "[warn] robots.txt not loaded: %v\n", err)
	}

	c := &Crawler{
		client:  client,
		robots:  robots,
		db:      db,
		csvW:    csvW,
		jsonlW:  jsonlW,
		rate:    time.NewTicker(time.Duration(rateMs) * time.Millisecond),
		allowed: allowFn,
	}
	defer c.rate.Stop()

	maxPages, err := c.findMaxPages()
	if err != nil {
		// Fallback to a safe upper bound
		fmt.Fprintf(os.Stderr, "[warn] cannot detect max pages: %v -> fallback 100\n", err)
		maxPages = 100
	}

	for p := 1; p <= maxPages; p++ {
		listURL := resolveListURL(p)
		fmt.Printf("[list] %s\n", listURL)

		html, err := c.get(listURL)
		if err != nil {
			fmt.Fprintf(os.Stderr, "[list] skip %s: %v\n", listURL, err)
			continue
		}

		links, err := extractDetailLinks(html)
		if err != nil {
			fmt.Fprintf(os.Stderr, "[list] parse %s: %v\n", listURL, err)
			continue
		}

		for _, durl := range links {
			done, _ := isVisited(c.db, durl)
			if done {
				continue
			}
			fmt.Printf("  [detail] %s\n", durl)

			meta, err := c.parseDetail(durl)
			if err != nil {
				fmt.Fprintf(os.Stderr, "  [detail] err %s: %v\n", durl, err)
				_ = mark(c.db, durl, "err", "", nil)
				continue
			}

			var pdfPath string
			if meta.PDFURL != "" {
				p, err := c.downloadPDF(meta.PDFURL, meta.Subject, meta.Title)
				if err != nil {
					fmt.Fprintf(os.Stderr, "  [pdf] %s: %v\n", meta.PDFURL, err)
				} else {
					pdfPath = p
					meta.PDFPath = p
				}
			}

			// Write CSV
			if err := c.csvW.Write([]string{
				meta.URL,
				meta.Title,
				meta.Subject,
				strings.Join(meta.Grades, ";"),
				strings.Join(meta.Tags, ";"),
				meta.PDFURL,
				pdfPath,
			}); err != nil {
				fmt.Fprintf(os.Stderr, "  [csv] %v\n", err)
			}
			c.csvW.Flush()

			// Write JSONL
			b, _ := json.Marshal(meta)
			if _, err := c.jsonlW.Write(b); err == nil {
				_, _ = c.jsonlW.WriteString("\n")
				_ = c.jsonlW.Flush()
			}

			_ = mark(c.db, durl, "ok", pdfPath, &meta)
		}
	}

	fmt.Println("Done.")
	return nil
}

func createSchema(db *sql.DB) error {
	_, err := db.Exec(`
CREATE TABLE IF NOT EXISTS visited(
  url TEXT PRIMARY KEY,
  status TEXT,
  pdf_path TEXT,
  meta_json TEXT
);`)
	return err
}

func isVisited(db *sql.DB, u string) (bool, error) {
	var s string
	err := db.QueryRow(`SELECT status FROM visited WHERE url = ?`, u).Scan(&s)
	if errors.Is(err, sql.ErrNoRows) {
		return false, nil
	}
	return err == nil && (s == "ok" || s == "skip"), err
}

func mark(db *sql.DB, u, status, pdf string, meta *Meta) error {
	var js string
	if meta != nil {
		b, _ := json.Marshal(meta)
		js = string(b)
	}
	_, err := db.Exec(`INSERT INTO visited(url,status,pdf_path,meta_json)
VALUES(?,?,?,?)
ON CONFLICT(url) DO UPDATE SET status=excluded.status, pdf_path=excluded.pdf_path, meta_json=excluded.meta_json`,
		u, status, pdf, js)
	return err
}

func isEmpty(f *os.File) bool {
	info, err := f.Stat()
	return err == nil && info.Size() == 0
}

func loadRobots(client *http.Client) (*robotstxt.RobotsData, func(rawURL string) bool, error) {
	u := baseURL + "/robots.txt"
	req, _ := http.NewRequest("GET", u, nil)
	req.Header.Set("User-Agent", userAgent)
	resp, err := client.Do(req)
	if err != nil {
		return nil, func(_ string) bool { return true }, err
	}
	defer resp.Body.Close()
	b, _ := io.ReadAll(resp.Body)
	rd, err := robotstxt.FromStatusAndBytes(resp.StatusCode, b)
	if err != nil {
		return nil, func(_ string) bool { return true }, err
	}
	g := rd.FindGroup(userAgent)
	if g == nil {
		g = rd.FindGroup("*")
	}
	allow := func(raw string) bool {
		if g == nil {
			return true
		}
		return g.Test(mustPath(raw))
	}
	return rd, allow, nil
}

func mustPath(raw string) string {
	u, err := url.Parse(raw)
	if err != nil {
		return raw
	}
	if u.Path == "" {
		return "/"
	}
	return u.Path
}

func (c *Crawler) get(rawURL string) (string, error) {
	// Respect robots.txt
	if c.allowed != nil && !c.allowed(rawURL) {
		return "", fmt.Errorf("blocked by robots.txt: %s", rawURL)
	}
	<-c.rate.C // rate limit
	req, _ := http.NewRequestWithContext(context.Background(), "GET", rawURL, nil)
	req.Header.Set("User-Agent", userAgent)
	resp, err := c.client.Do(req)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()
	if resp.StatusCode >= 400 {
		return "", fmt.Errorf("http %d", resp.StatusCode)
	}
	b, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", err
	}
	return string(b), nil
}

func (c *Crawler) findMaxPages() (int, error) {
	html, err := c.get(baseURL + listPath)
	if err != nil {
		return 0, err
	}
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return 0, err
	}
	max := 1
	re := regexp.MustCompile(`/all-downloads/page/(\d+)/?`)
	doc.Find(`a[href*="/all-downloads/page/"]`).Each(func(_ int, s *goquery.Selection) {
		if href, ok := s.Attr("href"); ok {
			m := re.FindStringSubmatch(href)
			if len(m) == 2 {
				if n := atoiSafe(m[1]); n > max {
					max = n
				}
			}
		}
	})
	return max, nil
}

func resolveListURL(p int) string {
	if p <= 1 {
		return baseURL + listPath
	}
	return fmt.Sprintf("%s/all-downloads/page/%d/", baseURL, p)
}

func extractDetailLinks(html string) ([]string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return nil, err
	}
	var links []string
	doc.Find("a[href]").Each(func(_ int, s *goquery.Selection) {
		href, _ := s.Attr("href")
		if href == "" {
			return
		}
		if isDetailURL(href) {
			links = append(links, toAbs(href))
		}
	})
	return uniq(links), nil
}

func isDetailURL(href string) bool {
	h := strings.ToLower(href)
	if strings.HasPrefix(h, "mailto:") || strings.HasPrefix(h, "javascript:") {
		return false
	}
	// Heuristics: worksheet/article detail paths
	for _, seg := range []string{
		"/worksheet/",
		"/worksheets/",
		"/find-",
		"/tracing-",
		"/sight-words/",
		"/vocabulary/",
		"/shapes/",
	} {
		if strings.Contains(h, seg) {
			return true
		}
	}
	return false
}

func toAbs(href string) string {
	u, err := url.Parse(href)
	if err != nil {
		return href
	}
	if u.IsAbs() {
		return href
	}
	base, _ := url.Parse(baseURL)
	return base.ResolveReference(u).String()
}

func uniq(ss []string) []string {
	m := make(map[string]struct{}, len(ss))
	out := make([]string, 0, len(ss))
	for _, s := range ss {
		if _, ok := m[s]; !ok {
			m[s] = struct{}{}
			out = append(out, s)
		}
	}
	return out
}

func atoiSafe(s string) int {
	n := 0
	for _, r := range s {
		if r >= '0' && r <= '9' {
			n = n*10 + int(r-'0')
		}
	}
	return n
}

func (c *Crawler) parseDetail(detailURL string) (Meta, error) {
	html, err := c.get(detailURL)
	if err != nil {
		return Meta{}, err
	}
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return Meta{}, err
	}

	// Title (prefer H1)
	title := strings.TrimSpace(doc.Find("h1").First().Text())
	if title == "" {
		title = fallbackTitle(detailURL)
	}

	meta := Meta{
		URL:   detailURL,
		Title: title,
	}

	// Try to read "Subject:" and "Grade:" in any <li> or info block
	doc.Find("li").Each(func(_ int, li *goquery.Selection) {
		text := strings.ToLower(strings.TrimSpace(li.Text()))
		if strings.HasPrefix(text, "subject") {
			val := strings.TrimSpace(afterColon(li.Text()))
			if val != "" {
				meta.Subject = val
			}
		}
		if strings.HasPrefix(text, "grade") {
			val := strings.TrimSpace(afterColon(li.Text()))
			if val != "" {
				meta.Grades = splitGrades(val)
			}
		}
	})

	// Collect tags from /tag/ or /document-tag/
	tagSet := map[string]struct{}{}
	doc.Find(`a[href*="/tag/"], a[href*="/document-tag/"]`).Each(func(_ int, a *goquery.Selection) {
		t := strings.TrimSpace(a.Text())
		if t != "" {
			tagSet[t] = struct{}{}
		}
	})
	for t := range tagSet {
		meta.Tags = append(meta.Tags, t)
	}

	// Find PDF link: anchor with .pdf or text contains "download"/"pdf"
	var candidates []string
	doc.Find("a[href]").Each(func(_ int, a *goquery.Selection) {
		h, _ := a.Attr("href")
		lowText := strings.ToLower(strings.TrimSpace(a.Text()))
		if strings.HasSuffix(strings.ToLower(h), ".pdf") {
			candidates = append(candidates, toAbs(h))
		}
		if strings.Contains(lowText, "download") || strings.Contains(lowText, "pdf") {
			candidates = append(candidates, toAbs(h))
		}
	})

	// Keep only same-domain candidates
	var sameDomain []string
	for _, u := range uniq(candidates) {
		pu, err := url.Parse(u)
		if err == nil && strings.HasSuffix(pu.Hostname(), "kiddoworksheets.com") {
			sameDomain = append(sameDomain, u)
		}
	}

	// Prefer direct .pdf
	for _, u := range sameDomain {
		if strings.HasSuffix(strings.ToLower(u), ".pdf") {
			meta.PDFURL = u
			break
		}
	}
	// If not found, try to probe the first candidate to see if it serves PDF
	if meta.PDFURL == "" && len(sameDomain) > 0 {
		if c.isPDF(sameDomain[0]) {
			meta.PDFURL = sameDomain[0]
		}
	}

	return meta, nil
}

func (c *Crawler) isPDF(u string) bool {
	// HEAD may be blocked; use GET with small body read
	body, ct, err := c.fetchHeadLike(u)
	if err != nil {
		return false
	}
	defer body.Close()
	return strings.Contains(strings.ToLower(ct), "application/pdf") || strings.HasSuffix(strings.ToLower(u), ".pdf")
}

func (c *Crawler) fetchHeadLike(u string) (io.ReadCloser, string, error) {
	if c.allowed != nil && !c.allowed(u) {
		return nil, "", fmt.Errorf("blocked by robots.txt: %s", u)
	}
	<-c.rate.C
	req, _ := http.NewRequest("GET", u, nil)
	req.Header.Set("User-Agent", userAgent)
	resp, err := c.client.Do(req)
	if err != nil {
		return nil, "", err
	}
	if resp.StatusCode >= 400 {
		resp.Body.Close()
		return nil, "", fmt.Errorf("http %d", resp.StatusCode)
	}
	ct := resp.Header.Get("Content-Type")
	// Return body so caller can close
	return resp.Body, ct, nil
}

func (c *Crawler) downloadPDF(pdfURL, subject, title string) (string, error) {
	// Build directory and filename
	subdir := "unspecified"
	if subject = strings.TrimSpace(subject); subject != "" {
		subdir = slugify(subject)
	}
	dir := filepath.Join(outDir, subdir)
	if err := os.MkdirAll(dir, 0o755); err != nil {
		return "", err
	}
	name := slugify(title)
	if !strings.HasSuffix(strings.ToLower(name), ".pdf") {
		name += ".pdf"
	}
	outPath := filepath.Join(dir, name)

	// Skip if exists and non-empty
	if fi, err := os.Stat(outPath); err == nil && fi.Size() > 0 {
		return outPath, nil
	}

	if c.allowed != nil && !c.allowed(pdfURL) {
		return "", fmt.Errorf("blocked by robots.txt: %s", pdfURL)
	}
	<-c.rate.C
	req, _ := http.NewRequest("GET", pdfURL, nil)
	req.Header.Set("User-Agent", userAgent)
	resp, err := c.client.Do(req)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()
	if resp.StatusCode >= 400 {
		return "", fmt.Errorf("http %d", resp.StatusCode)
	}
	ct := strings.ToLower(resp.Header.Get("Content-Type"))
	if !strings.Contains(ct, "pdf") && !strings.HasSuffix(strings.ToLower(pdfURL), ".pdf") {
		return "", fmt.Errorf("not a pdf (content-type: %s)", ct)
	}
	f, err := os.Create(outPath)
	if err != nil {
		return "", err
	}
	defer f.Close()

	if _, err := io.Copy(f, resp.Body); err != nil {
		return "", err
	}
	return outPath, nil
}

// -------------------- helpers --------------------

func slugify(s string) string {
	s = strings.ToLower(strings.TrimSpace(s))
	// Remove non-word (keep spaces & dashes)
	re := regexp.MustCompile(`[^\p{L}\p{N}\s-]+`)
	s = re.ReplaceAllString(s, "")
	// Collapse spaces/underscores to single dash
	re2 := regexp.MustCompile(`[\s_-]+`)
	s = re2.ReplaceAllString(s, "-")
	// Trim dash
	s = strings.Trim(s, "-")
	if len(s) > 120 {
		s = s[:120]
	}
	return s
}

func fallbackTitle(u string) string {
	pu, err := url.Parse(u)
	if err != nil {
		return u
	}
	parts := strings.Split(strings.Trim(pu.Path, "/"), "/")
	if len(parts) == 0 {
		return u
	}
	return strings.ReplaceAll(parts[len(parts)-1], "-", " ")
}

func afterColon(s string) string {
	if i := strings.IndexRune(s, ':'); i >= 0 {
		return s[i+1:]
	}
	return s
}

func splitGrades(s string) []string {
	parts := regexp.MustCompile(`[,/]|and`).Split(s, -1)
	var out []string
	for _, p := range parts {
		p = strings.TrimSpace(p)
		if p != "" {
			out = append(out, p)
		}
	}
	return out
}
